{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54131b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cc97ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-large-en-v1.5\", model_kwargs={\"device\": device}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4ef2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from pinecone import Pinecone\n",
    "\n",
    "load_dotenv()\n",
    "pc = Pinecone()\n",
    "index = pc.Index(\"faqs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011b5d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "vector_store = PineconeVectorStore(index=index, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2c9741",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store.similarity_search(\"forget password\", k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16372b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 5},\n",
    ")\n",
    "retriever.invoke(\"how can i stay healthy?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c2087a",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\"k\": 5, \"lambda_mult\": 0.5},\n",
    ")\n",
    "retriever.invoke(\"how can i stay healthy?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607b0cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "question = \"Forget\"\n",
    "llm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")\n",
    "retriever_from_llm = MultiQueryRetriever.from_llm(\n",
    "    retriever=vector_store.as_retriever(\n",
    "        search_type=\"mmr\", search_kwargs={\"k\": 4, \"lambda_mult\": 0.5}\n",
    "    ),\n",
    "    llm=llm,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3d64f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52125519",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3e89e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b688c774",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b722506",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d7e933",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-large-en-v1.5\", model_kwargs={\"device\": device}\n",
    ")\n",
    "\n",
    "\n",
    "from pinecone import Pinecone\n",
    "\n",
    "pc = Pinecone()\n",
    "index = pc.Index(\"faqs\")\n",
    "\n",
    "\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "vector_store = PineconeVectorStore(index=index, embedding=embeddings)\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "llm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")\n",
    "retriever = MultiQueryRetriever.from_llm(\n",
    "    retriever=vector_store.as_retriever(\n",
    "        search_type=\"mmr\", search_kwargs={\"k\": 4, \"lambda_mult\": 0.5}\n",
    "    ),\n",
    "    llm=llm,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc49e628",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"forget\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7204903",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fd3edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use three sentences maximum and keep the answer as concise as possible.\n",
    "Always say \"thanks for asking!\" at the end of the answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "custom_rag_prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0af22c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9010b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3663c55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0bab70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615c3e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use three sentences maximum and keep the answer as concise as possible.\n",
    "Always say \"thanks for asking!\" at the end of the answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "custom_rag_prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28e87ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from support_hub.model_hub import get_model\n",
    "\n",
    "model = get_model()\n",
    "from dotenv import load_dotenv\n",
    "from pinecone import Pinecone\n",
    "\n",
    "load_dotenv()\n",
    "pc = Pinecone()\n",
    "index = pc.Index(\"faqs\")\n",
    "import torch\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-large-en-v1.5\", model_kwargs={\"device\": device}\n",
    ")\n",
    "vector_store = PineconeVectorStore(index=index, embedding=embeddings)\n",
    "from support_hub.retriever_factory import create_multi_query_retriever\n",
    "\n",
    "retriever = create_multi_query_retriever(vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a5ff58",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.invoke(\"forget\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25e1bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_rag_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f83b57f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e2ccb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87196f64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ff80e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521767d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666daf79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f8187d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12986bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443b6040",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d19dad9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a26a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.invoke({\"question\": \"How can I reset my password?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27153cff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8069edda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c4f873",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4a551e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4de04b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "llm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb2b5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "tagging_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Extract the desired information from the following passage.\n",
    "\n",
    "Only extract the properties mentioned in the 'Classification' function.\n",
    "\n",
    "Passage:\n",
    "{input}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "class Classification(BaseModel):\n",
    "    sentiment: str = Field(..., enum=[\"happy\", \"neutral\", \"sad\"])\n",
    "    aggressiveness: int = Field(\n",
    "        ...,\n",
    "        description=\"describes how aggressive the statement is, the higher the number the more aggressive\",\n",
    "        enum=[1, 2, 3, 4, 5],\n",
    "    )\n",
    "    language: str = Field(\n",
    "        ..., enum=[\"spanish\", \"english\", \"french\", \"german\", \"italian\"]\n",
    "    )\n",
    "\n",
    "\n",
    "# Structured LLM\n",
    "structured_llm = llm.with_structured_output(Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abef305",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"ff\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75884b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "llm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da867596",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "class Classification(BaseModel):\n",
    "    sentiment: str = Field(..., enum=[\"happy\", \"neutral\", \"sad\"])\n",
    "    aggressiveness: int = Field(\n",
    "        ...,\n",
    "        description=\"describes how aggressive the statement is, the higher the number the more aggressive\",\n",
    "        enum=[1, 2, 3, 4, 5],\n",
    "    )\n",
    "    language: str = Field(\n",
    "        ..., enum=[\"spanish\", \"english\", \"french\", \"german\", \"italian\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c10157",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagging_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Extract the desired information from the following passage.\n",
    "\n",
    "Only extract the properties mentioned in the 'Classification' function.\n",
    "\n",
    "Passage:\n",
    "{input}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "a = llm.invoke(f\"i forget password you are bad person\\n out should be in {Classification.model_json_schema()} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf09a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5dc15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Classification(**{\n",
    "  \"sentiment\": \"sad\",\n",
    "  \"aggressiveness\": 5,\n",
    "  \"language\": \"english\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e142ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fa296b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2fe7ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74478819",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "# from langchain.output_parsers import EnumOutputParser\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from pydantic import BaseModel, Field, model_validator\n",
    "\n",
    "model = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")\n",
    "\n",
    "from enum import Enum\n",
    "\n",
    "from langchain.output_parsers import OutputFixingParser\n",
    "from pydantic import BaseModel, Field, conint\n",
    "\n",
    "\n",
    "class CategoriesProbabilities(BaseModel):\n",
    "    Desktop: float = Field(\n",
    "        ...,\n",
    "        description=\"Probability that the ticket relates to general desktop issues, including software, settings, or performance.\",\n",
    "    )\n",
    "    Desktop_Application: float = Field(\n",
    "        ...,\n",
    "        description=\"Probability that the ticket is about problems with installed desktop applications (e.g., MS Office, browsers).\",\n",
    "    )\n",
    "    Desktop_Asset_Management: float = Field(\n",
    "        ...,\n",
    "        description=\"Probability that the ticket concerns desktop asset tracking, inventory, or allocation.\",\n",
    "    )\n",
    "    Desktop_Hardware: float = Field(\n",
    "        ...,\n",
    "        description=\"Probability that the ticket is related to physical hardware issues such as monitor, keyboard, or peripherals.\",\n",
    "    )\n",
    "    Desktop_Login_Issue: float = Field(\n",
    "        ...,\n",
    "        description=\"Probability that the ticket is about desktop login problems, like incorrect credentials or account lockouts.\",\n",
    "    )\n",
    "    Desktop_Storage: float = Field(\n",
    "        ...,\n",
    "        description=\"Probability that the ticket involves local storage issues, such as low disk space or drive access problems.\",\n",
    "    )\n",
    "\n",
    "    Email: float = Field(\n",
    "        ...,\n",
    "        description=\"Probability that the ticket relates to general email issues, including access, sync, or configuration problems.\",\n",
    "    )\n",
    "    Email_Email_Create: float = Field(\n",
    "        ...,\n",
    "        description=\"Probability that the ticket is a request to create a new company email ID or mailbox.\",\n",
    "    )\n",
    "    Email_Whitelist_Email: float = Field(\n",
    "        ...,\n",
    "        description=\"Probability that the ticket requests to whitelist an email address or domain to prevent blocking or spam issues.\",\n",
    "    )\n",
    "\n",
    "    Internet: float = Field(\n",
    "        ...,\n",
    "        description=\"Probability that the ticket relates to internet connectivity issues, such as no access, slow speed, or frequent disconnections.\",\n",
    "    )\n",
    "    Network: float = Field(\n",
    "        ...,\n",
    "        description=\"Probability that the ticket involves internal network problems, including LAN/Wi-Fi issues or access to shared resources.\",\n",
    "    )\n",
    "    International_Calling: float = Field(\n",
    "        ...,\n",
    "        description=\"Probability that the ticket relates to enabling, disabling, or troubleshooting international calling services.\",\n",
    "    )\n",
    "\n",
    "    Server: float = Field(\n",
    "        ...,\n",
    "        description=\"Probability that the ticket is about general server-related issues or requests not covered by subcategories.\",\n",
    "    )\n",
    "    Server_Application: float = Field(\n",
    "        ...,\n",
    "        description=\"Probability that the ticket concerns applications hosted on servers, including downtime or access errors.\",\n",
    "    )\n",
    "    Server_Client_Servers: float = Field(\n",
    "        ...,\n",
    "        description=\"Probability that the ticket relates to client-specific servers, configurations, or deployment problems.\",\n",
    "    )\n",
    "\n",
    "    Storage: float = Field(\n",
    "        ...,\n",
    "        description=\"Probability that the ticket involves shared or cloud storage issues (e.g., OneDrive, NAS, shared drives).\",\n",
    "    )\n",
    "    Printing: float = Field(\n",
    "        ...,\n",
    "        description=\"Probability that the ticket is about printing issues, such as printer detection, queue errors, or print quality problems.\",\n",
    "    )\n",
    "\n",
    "    Website_Blocked: float = Field(\n",
    "        ...,\n",
    "        description=\"Probability that the ticket requests access to a blocked website or restricted web content.\",\n",
    "    )\n",
    "    New_Service_Request: float = Field(\n",
    "        ...,\n",
    "        description=\"Probability that the ticket is a request for a new IT service, software installation, or hardware provisioning.\",\n",
    "    )\n",
    "\n",
    "    CSG_PDG: float = Field(\n",
    "        ...,\n",
    "        description=\"Probability that the ticket is related to Corporate Services Group â€“ Product Development Group internal tools or systems.\",\n",
    "    )\n",
    "    Others: float = Field(\n",
    "        ...,\n",
    "        description=\"Probability that the ticket does not fall into any predefined categories and needs manual review or reassignment.\",\n",
    "    )\n",
    "\n",
    "    def top_category(self) -> tuple[str, float]:\n",
    "        data = self.model_dump()  # Pydantic v2\n",
    "        return max(data.items(), key=lambda kv: kv[1])\n",
    "\n",
    "\n",
    "class Priority(Enum):\n",
    "    VeryHigh = 1\n",
    "    High = 2\n",
    "    Normal = 3\n",
    "    Low = 4\n",
    "    VeryLow = 5\n",
    "\n",
    "\n",
    "class TicketPrediction(BaseModel):\n",
    "    categories_probabilities: CategoriesProbabilities = Field(\n",
    "        ..., description=\"Probability scores for each ticket category.\"\n",
    "    )\n",
    "    priority: Priority = Field(\n",
    "        ...,\n",
    "        description=(\n",
    "            \"Priority of the ticket, from 1 to 5:\\n\"\n",
    "            \"1 = Very High\\n\"\n",
    "            \"2 = High\\n\"\n",
    "            \"3 = Normal\\n\"\n",
    "            \"4 = Low\\n\"\n",
    "            \"5 = Very Low\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = PydanticOutputParser(pydantic_object=TicketPrediction)\n",
    "\n",
    "new_parser = OutputFixingParser.from_llm(parser=parser, llm=model)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "You are an IT support ticket classifier.\n",
    "\n",
    "Given the ticket description, provide the following:\n",
    "\n",
    "1. **categories_probabilities**: a probability score (in percentage) for each ticket category that represents how likely the ticket belongs to that category.\n",
    "   - All probabilities must sum up to 100%.\n",
    "   - No two categories should have the same probability score.\n",
    "\n",
    "2. **priority**: one of the following Enum values (VeryHigh = 1, High = 2, Normal = 3, Low = 4, VeryLow = 5), representing how important it is to resolve the issue quickly so that there are no work blockers or wasted time.\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "Ticket Description:\n",
    "\"{query}\"\n",
    "\"\"\",\n",
    "    input_variables=[\"query\"],\n",
    "    # partial_variables={\"format_instructions\": new_parser.get_format_instructions()},\n",
    "    partial_variables={\"format_instructions\": new_parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "\n",
    "# And a query intended to prompt a language model to populate the data structure.\n",
    "prompt_and_model = prompt | model\n",
    "output = prompt_and_model.invoke({\"query\": \"my active directory is locked\"})\n",
    "opt = parser.invoke(output)\n",
    "print(opt.categories_probabilities.top_category())\n",
    "print(opt.priority)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7125ed16",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.categories_probabilities.top_category()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7589eb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompt.invoke({\"query\": \"it is very good  my  bloody son.\"}).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc14148",
   "metadata": {},
   "outputs": [],
   "source": [
    "from support_hub.ticket_classifier.ticket_classification import run_ticket_classification\n",
    "\n",
    "await run_ticket_classification(\"create new s3 bucket for me?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b21758a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from support_hub.ticket_classifier.ticket_classification import run_ticket_classification\n",
    "\n",
    "await run_ticket_classification(\"my active directory is locked\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071ff3ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10ea72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "import json\n",
    "\n",
    "async def fetch_skills(session, base_url):\n",
    "    \"\"\"\n",
    "    Asynchronously fetch skills data from the API endpoint.\n",
    "    \n",
    "    Args:\n",
    "        session (aiohttp.ClientSession): The aiohttp session for making requests.\n",
    "        base_url (str): The base URL of the API (e.g., 'http://example.com').\n",
    "    \n",
    "    Returns:\n",
    "        dict: The parsed JSON response.\n",
    "    \"\"\"\n",
    "    url = f\"{base_url}/api/skills/\"\n",
    "    async with session.get(url) as response:\n",
    "        # if response.status == 200:\n",
    "            return await response.json()\n",
    "        # else:\n",
    "        #     raise Exception(f\"Failed to fetch data: HTTP {response.status}\")\n",
    "\n",
    "async with aiohttp.ClientSession() as session:\n",
    "       \n",
    "            data = await fetch_skills(session, \"http://192.168.71.115\")\n",
    "            print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a9fd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiohttp\n",
    "\n",
    "async def fetch_roles(session, base_url):\n",
    "    \"\"\"\n",
    "    Asynchronously fetch skills data from the API endpoint.\n",
    "    \n",
    "    Args:\n",
    "        session (aiohttp.ClientSession): The aiohttp session for making requests.\n",
    "        base_url (str): The base URL of the API (e.g., 'http://example.com').\n",
    "    \n",
    "    Returns:\n",
    "        dict: The parsed JSON response.\n",
    "    \"\"\"\n",
    "    url = f\"{base_url}/api/skills/\"\n",
    "    async with session.get(url) as response:\n",
    "        if response.status == 200:\n",
    "            return await response.json()\n",
    "        else:\n",
    "            raise Exception(f\"Failed to fetch data: HTTP {response.status}\")\n",
    "\n",
    "async def list_roles():\n",
    "    base_url = \"http://192.168.71.115\"  # Replace with your actual base URL\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        try:\n",
    "            data = await fetch_roles(session, base_url)\n",
    "            # Example: Access the roles\n",
    "            roles = [item[\"role\"] for item in data[\"data\"]]\n",
    "            return {\"roles\": roles}\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "await list_roles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6d6626",
   "metadata": {},
   "outputs": [],
   "source": [
    "roles = await list_roles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b5d780",
   "metadata": {},
   "outputs": [],
   "source": [
    "roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffdfd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiohttp\n",
    "\n",
    "\n",
    "class RoleFetcher:\n",
    "    \"\"\"\n",
    "    A class to asynchronously fetch roles from the API.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_url: str = \"http://192.168.71.115\"):\n",
    "        \"\"\"\n",
    "        Initialize the RoleFetcher with the base URL of the API.\n",
    "\n",
    "        Args:\n",
    "            base_url (str): The base URL of the API (e.g., 'http://192.168.71.115').\n",
    "        \"\"\"\n",
    "        self.base_url = base_url\n",
    "\n",
    "    async def fetch_roles(self):\n",
    "        \"\"\"\n",
    "        Asynchronously fetch roles from the API endpoint.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing the list of roles.\n",
    "\n",
    "        Raises:\n",
    "            Exception: If the API request fails.\n",
    "        \"\"\"\n",
    "        url = f\"{self.base_url}/api/skills/\"\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            async with session.get(url) as response:\n",
    "                if response.status == 200:\n",
    "                    data = await response.json()\n",
    "                    roles = [item[\"role\"] for item in data[\"data\"]]\n",
    "                    return {\"roles\": roles}\n",
    "                else:\n",
    "                    raise Exception(f\"Failed to fetch data: HTTP {response.status}\")\n",
    "\n",
    "    async def list_roles(self):\n",
    "        \"\"\"\n",
    "        Asynchronously list roles from the API.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing the list of roles.\n",
    "\n",
    "        Raises:\n",
    "            Exception: If the API request fails.\n",
    "        \"\"\"\n",
    "        return await self.fetch_roles()\n",
    "\n",
    "\n",
    "fetcher = RoleFetcher()\n",
    "\n",
    "await fetcher.list_roles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0f013c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from support_hub.ticket_classifier.models.utils import RoleFetcher\n",
    "\n",
    "\n",
    "async def get_roles():\n",
    "    fetcher = RoleFetcher()\n",
    "    try:\n",
    "        result = await fetcher.list_roles()\n",
    "        return result.get(\"roles\", [])\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "a = await get_roles()\n",
    "\n",
    "class Roles(BaseModel):\n",
    "    role_names: list[str] = Field(default=a, description=\"Names of the roles\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15122592",
   "metadata": {},
   "outputs": [],
   "source": [
    "Roles(role_names=a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ef980e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a54397e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaae3c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from support_hub.ticket_classifier.parsers.output_parsers import get_pydantic_role_parser\n",
    "\n",
    "p = get_pydantic_role_parser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a56fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from support_hub.ticket_classifier.prompts.ticket_prompt import get_ticket_classification_prompt_roles\n",
    "\n",
    "print(get_ticket_classification_prompt_roles(p.get_format_instructions()).invoke({\"ticket\": \"my active directory is locked\"}).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed915ddb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfa48c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c01d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.documents import Document\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "\n",
    "from support_hub.model_hub import get_model\n",
    "from support_hub.retriever_factory import create_multi_query_retriever\n",
    "\n",
    "llm = get_model()\n",
    "# prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Keep the answer as concise as possible.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-large-en-v1.5\", model_kwargs={\"device\": device}\n",
    ")\n",
    "from dotenv import load_dotenv\n",
    "from pinecone import Pinecone\n",
    "\n",
    "load_dotenv()\n",
    "pc = Pinecone()\n",
    "index = pc.Index(\"faqs\")\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "vector_store = PineconeVectorStore(index=index, embedding=embeddings)\n",
    "\n",
    "\n",
    "# Define state for application\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "    answer_found: bool\n",
    "\n",
    "\n",
    "# Define application steps\n",
    "def retrieve(state: State):\n",
    "    # retrieved_docs = vector_store.similarity_search(state[\"question\"])\n",
    "    retrieved_docs = create_multi_query_retriever(vector_store=vector_store).invoke(\n",
    "        state[\"question\"]\n",
    "    )\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "\n",
    "from support_hub.context_evaluateor import evaluate_relevance\n",
    "\n",
    "\n",
    "async def generate(state: State):\n",
    "    docs_content = (\n",
    "        \"\\n```\\n\" + \"\\n\\n\".join(doc.page_content for doc in state[\"context\"]) + \"\\n```\"\n",
    "    )\n",
    "    # print(docs_content)\n",
    "\n",
    "    evaluated_class = await evaluate_relevance(\n",
    "        query=state[\"question\"], retrieved_context=docs_content\n",
    "    )\n",
    "    # print(\"end=  \", evaluated_class.get(\"reason\"))\n",
    "    if evaluated_class.get(\"is_relevant\") is False:\n",
    "        return {\"answer_found\": False}\n",
    "\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer_found\": True, \"answer\": response.content}\n",
    "\n",
    "\n",
    "# Compile application and test\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()\n",
    "answer = await graph.ainvoke({\"question\": \"How can I reset my password??\"})\n",
    "print(answer.get(\"answer_found\"))\n",
    "if answer.get(\"answer_found\"):\n",
    "    print(answer[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea2b101",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Following are the retrieved context. give answer according to these context only\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68b973e",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = graph.invoke({\"question\": \"How can I reset my password?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64156540",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer[\"context\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc6ba62",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc28117a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4dac286",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f350aff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3551d72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5877d71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbc4012",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700896c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "\n",
    "import torch\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.language_models.base import BaseLanguageModel\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.vectorstores import VectorStore\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langgraph.graph import START, StateGraph\n",
    "from pinecone import Pinecone\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from support_hub.context_evaluateor import evaluate_relevance\n",
    "from support_hub.model_hub import get_model\n",
    "from support_hub.retriever_factory import create_multi_query_retriever\n",
    "\n",
    "\n",
    "# State definition\n",
    "class RAGState(TypedDict):\n",
    "    \"\"\"State schema for RAG pipeline.\"\"\"\n",
    "\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "    answer_found: bool\n",
    "\n",
    "\n",
    "class RAGConfig:\n",
    "    \"\"\"Configuration for RAG pipeline components.\"\"\"\n",
    "\n",
    "    # Prompt template\n",
    "    ANSWER_TEMPLATE = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Keep the answer as concise as possible.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "    # Embedding model configuration\n",
    "    EMBEDDING_MODEL_NAME = \"BAAI/bge-large-en-v1.5\"\n",
    "\n",
    "    # Pinecone configuration\n",
    "    PINECONE_INDEX_NAME = \"faqs\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_model_name: Optional[str] = None,\n",
    "        pinecone_index_name: Optional[str] = None,\n",
    "        answer_template: Optional[str] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize RAG configuration.\n",
    "\n",
    "        Args:\n",
    "            embedding_model_name: Name of HuggingFace embedding model.\n",
    "            pinecone_index_name: Name of Pinecone index.\n",
    "            answer_template: Custom prompt template for answer generation.\n",
    "        \"\"\"\n",
    "        self.embedding_model_name = embedding_model_name or self.EMBEDDING_MODEL_NAME\n",
    "        self.pinecone_index_name = pinecone_index_name or self.PINECONE_INDEX_NAME\n",
    "        self.answer_template = answer_template or self.ANSWER_TEMPLATE\n",
    "\n",
    "\n",
    "class RAGPipeline:\n",
    "    \"\"\"End-to-end RAG pipeline using LangGraph.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vector_store: Optional[VectorStore] = None,\n",
    "        llm: Optional[BaseLanguageModel] = None,\n",
    "        config: Optional[RAGConfig] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize RAG pipeline.\n",
    "\n",
    "        Args:\n",
    "            vector_store: Vector store for document retrieval. If None, creates Pinecone store.\n",
    "            llm: Language model for generation. If None, uses default from model_hub.\n",
    "            config: RAG configuration. If None, uses default configuration.\n",
    "        \"\"\"\n",
    "        self.config = config or RAGConfig()\n",
    "        self.llm = llm or get_model()\n",
    "        self.vector_store = vector_store or self._create_default_vector_store()\n",
    "        self.prompt = PromptTemplate.from_template(self.config.answer_template)\n",
    "        self.graph = self._build_graph()\n",
    "\n",
    "    def _create_default_vector_store(self) -> VectorStore:\n",
    "        \"\"\"Create default Pinecone vector store with HuggingFace embeddings.\"\"\"\n",
    "        load_dotenv()\n",
    "\n",
    "        # Set device for embeddings\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        # Initialize embeddings\n",
    "        embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=self.config.embedding_model_name, model_kwargs={\"device\": device}\n",
    "        )\n",
    "\n",
    "        # Initialize Pinecone\n",
    "        pc = Pinecone()\n",
    "        index = pc.Index(self.config.pinecone_index_name)\n",
    "\n",
    "        return PineconeVectorStore(index=index, embedding=embeddings)\n",
    "\n",
    "    def _retrieve(self, state: RAGState) -> dict:\n",
    "        \"\"\"\n",
    "        Retrieve relevant documents for the question.\n",
    "\n",
    "        Args:\n",
    "            state: Current RAG state.\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with retrieved context documents.\n",
    "        \"\"\"\n",
    "        retriever = create_multi_query_retriever(vector_store=self.vector_store)\n",
    "        retrieved_docs = retriever.invoke(state[\"question\"])\n",
    "        return {\"context\": retrieved_docs}\n",
    "\n",
    "    async def _generate(self, state: RAGState) -> dict:\n",
    "        \"\"\"\n",
    "        Generate answer from retrieved context.\n",
    "\n",
    "        Args:\n",
    "            state: Current RAG state with retrieved context.\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with answer_found flag and answer content.\n",
    "        \"\"\"\n",
    "        # Format context documents\n",
    "        docs_content = (\n",
    "            \"\\n```\"\n",
    "            + \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "            + \"\\n```\"\n",
    "        )\n",
    "\n",
    "        # Evaluate context relevance\n",
    "        evaluated_class = await evaluate_relevance(\n",
    "            query=state[\"question\"], retrieved_context=docs_content\n",
    "        )\n",
    "\n",
    "        # Return early if context is not relevant\n",
    "        if not evaluated_class.get(\"is_relevant\"):\n",
    "            return {\"answer_found\": False}\n",
    "\n",
    "        # Generate answer\n",
    "        messages = self.prompt.invoke(\n",
    "            {\"question\": state[\"question\"], \"context\": docs_content}\n",
    "        )\n",
    "        response = self.llm.invoke(messages)\n",
    "\n",
    "        return {\"answer_found\": True, \"answer\": response.content}\n",
    "\n",
    "    def _build_graph(self) -> StateGraph:\n",
    "        \"\"\"\n",
    "        Build LangGraph pipeline.\n",
    "\n",
    "        Returns:\n",
    "            Compiled StateGraph for RAG pipeline.\n",
    "        \"\"\"\n",
    "        graph_builder = StateGraph(RAGState).add_sequence(\n",
    "            [self._retrieve, self._generate]\n",
    "        )\n",
    "        graph_builder.add_edge(START, \"_retrieve\")\n",
    "        return graph_builder.compile()\n",
    "\n",
    "    async def ainvoke(self, question: str) -> dict:\n",
    "        \"\"\"\n",
    "        Asynchronously invoke RAG pipeline.\n",
    "\n",
    "        Args:\n",
    "            question: User question to answer.\n",
    "\n",
    "        Returns:\n",
    "            Dictionary containing answer_found flag and answer content.\n",
    "        \"\"\"\n",
    "        result = await self.graph.ainvoke({\"question\": question})\n",
    "        return {\n",
    "            \"answer_found\": result.get(\"answer_found\", False),\n",
    "            \"answer\": result.get(\"answer\"),\n",
    "        }\n",
    "\n",
    "    def invoke(self, question: str) -> dict:\n",
    "        \"\"\"\n",
    "        Synchronously invoke RAG pipeline.\n",
    "\n",
    "        Args:\n",
    "            question: User question to answer.\n",
    "\n",
    "        Returns:\n",
    "            Dictionary containing answer_found flag and answer content.\n",
    "        \"\"\"\n",
    "        result = self.graph.invoke({\"question\": question})\n",
    "        return {\n",
    "            \"answer_found\": result.get(\"answer_found\", False),\n",
    "            \"answer\": result.get(\"answer\"),\n",
    "        }\n",
    "\n",
    "    async def stream(self, question: str):\n",
    "        \"\"\"\n",
    "        Stream RAG pipeline execution with intermediate states.\n",
    "\n",
    "        Args:\n",
    "            question: User question to answer.\n",
    "\n",
    "        Yields:\n",
    "            Intermediate states during pipeline execution.\n",
    "        \"\"\"\n",
    "        async for state in self.graph.astream({\"question\": question}):\n",
    "            yield state\n",
    "\n",
    "\n",
    "# Factory function for easy instantiation\n",
    "async def create_rag_pipeline(\n",
    "    ticket: str,\n",
    "    vector_store: Optional[VectorStore] = None,\n",
    "    llm: Optional[BaseLanguageModel] = None,\n",
    "    config: Optional[RAGConfig] = None,\n",
    ") -> RAGPipeline:\n",
    "    \"\"\"\n",
    "    Create a RAG pipeline instance.\n",
    "\n",
    "    Args:\n",
    "        vector_store: Custom vector store. If None, uses default Pinecone.\n",
    "        llm: Custom language model. If None, uses default from model_hub.\n",
    "        config: Custom RAG configuration. If None, uses defaults.\n",
    "\n",
    "    Returns:\n",
    "        Configured RAGPipeline instance.\n",
    "    \"\"\"\n",
    "    pipeline = RAGPipeline(vector_store=vector_store, llm=llm, config=config)\n",
    "    result = await pipeline.ainvoke(ticket)\n",
    "    return {\"answer_found\": result.get(\"answer_found\"), \"answer\": result.get(\"answer\")}\n",
    "\n",
    "\n",
    "# Create pipeline with defaults\n",
    "pipeline = await create_rag_pipeline(\"my active directory is locked\")\n",
    "\n",
    "# # Get answer\n",
    "# result = await pipeline.ainvoke(\"hahaha\")\n",
    "\n",
    "# if result[\"answer_found\"]:\n",
    "#     print(f\"Answer: {result['answer']}\")\n",
    "# else:\n",
    "#     print(\"No relevant answer found in knowledge base.\")\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0979d4bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404aba02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967e8fe6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c33c8347",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/d/Sourabh_critical/ids-hackathon/backend/ai/.venv/lib/python3.12/site-packages/langchain_pinecone/__init__.py:3: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from langchain_pinecone.vectorstores import Pinecone, PineconeVectorStore\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1763109632.863861  235497 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n",
      "E0000 00:00:1763109634.378801  235497 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n",
      "E0000 00:00:1763109644.240134  235497 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer_found': False, 'answer': None}\n"
     ]
    }
   ],
   "source": [
    "from support_hub.rag_pipeline import create_rag_pipeline\n",
    "print(create_rag_pipeline(\"my active directory is locked\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a42e3f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1763109046.195574  232672 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n",
      "E0000 00:00:1763109046.219038  232672 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n",
      "E0000 00:00:1763109053.736055  232672 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'answer_found': False, 'answer': None}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_rag_pipeline(\"i forget my password\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "097043ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1763111763.224533  235497 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n",
      "E0000 00:00:1763111763.243450  235497 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n",
      "E0000 00:00:1763111778.645577  235497 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To configure your FortiClient VPN gateway settings, please follow these steps:\n",
      "\n",
      "1.  Open FortiClient and navigate to the VPN settings.\n",
      "2.  Click 'Add a new connection' to create a new VPN gateway.\n",
      "3.  Enter the connection name, description, and remote gateway details as required.\n",
      "4.  Save the settings and attempt to connect to the VPN.\n",
      "\n",
      "**Important Note:** If the VPN does not connect, please try removing the gateways one by one and testing the connection after each removal. Once you successfully connect, you can then add the gateways back again.\n"
     ]
    }
   ],
   "source": [
    "# print(create_rag_pipeline(\"I want to configure FortiClient VPN gateway settings\").get(\"answer\"))\n",
    "print(create_rag_pipeline(\"\"\"FortiClient VPN Configuration Request\\n\\nHi IT Team,\n",
    "\n",
    "I want to configure FortiClient VPN gateway settings\"\n",
    "\n",
    "Thanks,\n",
    "Davinder singh\n",
    "sr. software Engineer\n",
    "\n",
    "\"\"\").get(\"answer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd1c672",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d56de33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
